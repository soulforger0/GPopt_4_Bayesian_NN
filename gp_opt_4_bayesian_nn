# -*- coding: utf-8 -*-
import theano
floatX = theano.config.floatX
import pymc3 as pm
import theano.tensor as T
import sklearn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from warnings import filterwarnings
filterwarnings('ignore')
sns.set_style('white')
from sklearn import datasets
from sklearn.preprocessing import scale
from sklearn.cross_validation import train_test_split
from sklearn.datasets import make_moons, make_circles

import skopt
from skopt import gp_minimize, forest_minimize
from skopt.space import Real, Categorical, Integer
from skopt.plots import plot_convergence
from skopt.plots import plot_objective, plot_evaluations
#from skopt.plots import plot_histogram, plot_objective_2D
from skopt.utils import use_named_args



X, Y = make_circles(noise=0.1, factor=0.3, random_state=1, n_samples=4000, shuffle=True)
#X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)
X = scale(X)
X = X.astype(floatX)
Y = Y.astype(floatX)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)

fig, ax = plt.subplots()
ax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')
ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')
sns.despine(); ax.legend()
ax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');


# model creation function
def construct_nn(ann_input, ann_output, num_dense_nodes):
    n_hidden = num_dense_nodes

    # Initialize random weights between each layer
    init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)
    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)
    init_out = np.random.randn(n_hidden).astype(floatX)

    with pm.Model() as neural_network:
        # Weights from input to hidden layer
        weights_in_1 = pm.Normal('w_in_1', 0, sd=1,
                                 shape=(X.shape[1], n_hidden),
                                 testval=init_1)

        # Weights from 1st to 2nd layer
#        weights_1_2 = pm.Normal('w_1_2', 0, sd=1,
#                                shape=(n_hidden, n_hidden),
#                                testval=init_2)

        # Weights from hidden layer to output
        weights_2_out = pm.Normal('w_2_out', 0, sd=1,
                                  shape=(n_hidden,),
                                  testval=init_out)

        # Build neural-network using tanh activation function
        act_1 = pm.math.sigmoid(pm.math.dot(ann_input,
                                         weights_in_1))
#        act_2 = pm.math.sigmoid(pm.math.dot(act_1,
#                                         weights_1_2))
#        act_2 = pm.math.tanh(pm.math.dot(act_1,
#                                         weights_1_2))
#        act_out = pm.math.sigmoid(pm.math.dot(act_2,
#                                              weights_2_out))
        act_out = pm.math.sigmoid(pm.math.dot(act_1,
                                              weights_2_out))
        # Binary classification -> Bernoulli likelihood
        out = pm.Bernoulli('out',
                           act_out,
                           observed=ann_output,
                           total_size=Y_train.shape[0] # IMPORTANT for minibatches
                          )
    return neural_network




#### HYPER PREM TUNNING #####
dim_num_dense_nodes  = Integer(low=2, high=20, name='num_dense_nodes')
dimensions = [dim_num_dense_nodes]

best_accuracy  = 0.0
# creat fitness fucntion
@use_named_args(dimensions=dimensions)
def fitness(num_dense_nodes):
    # show current hyper prem
    print('\nnum_dense_nodes:', num_dense_nodes)
    ann_input = theano.shared(X_train)
    ann_output = theano.shared(Y_train)
   # Create the neural network with these hyper-parameters.
    nn_model = construct_nn(ann_input, ann_output, num_dense_nodes=num_dense_nodes)



    with nn_model:
        #inference = pm.ADVI()
        #approx = pm.fit(n=3000, method=inference)
        step = pm.Metropolis()
        trace = pm.sample(1000, step=step)


    ann_input.set_value(X_test)
    ann_output.set_value(Y_test)

    # Creater posterior predictive samples
    ppc = pm.sample_ppc(trace, model=nn_model, samples=1000)
    pred = ppc['out'].mean(axis=0) > 0.5
    
    model_accurcy = (Y_test == pred).mean() * 100
    print('Accuracy = {}%'.format(model_accurcy))
    
    global best_accuracy
    # If the classification accuracy of the saved model is improved ...
    if model_accurcy > best_accuracy:
        # Update the classification accuracy.
        best_accuracy = model_accurcy

    return -model_accurcy

default_parameters = [2]

# only for test
fitness(x=default_parameters)

# start searching hyper prem with gp as surragate function
search_result = gp_minimize(func=fitness,
                            dimensions=dimensions,
                            acq_func='EI', # Expected Improvement.
                            n_calls=11,
                            x0=default_parameters)












# Trick: Turn inputs and outputs into shared variables.
# It's still the same thing, but we can later change the values of the shared variable
# (to switch in the test-data later) and pymc3 will just use the new data.
# Kind-of like a pointer we can redirect.
# For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html
ann_input = theano.shared(X_train)
ann_output = theano.shared(Y_train)
neural_network = construct_nn(ann_input, ann_output)


with neural_network:
    #inference = pm.ADVI()
    #approx = pm.fit(n=3000, method=inference)
    step = pm.Metropolis()
    trace = pm.sample(1000, step=step)


# Replace shared variables with testing set
# (note that using this trick we could be streaming ADVI for big data)
ann_input.set_value(X_test)
ann_output.set_value(Y_test)






# Creater posterior predictive samples
ppc = pm.sample_ppc(trace, model=neural_network, samples=1000)
pred = ppc['out'].mean(axis=0) > 0.5

model_accurcy = (Y_test == pred).mean() * 100

plt.scatter(X_test[pred==0, 0], X_test[pred==0, 1])
plt.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')
plt.title('Predicted labels in testing set')

print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))




















