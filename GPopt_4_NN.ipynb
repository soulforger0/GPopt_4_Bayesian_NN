{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "#from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = make_circles(noise=0.1, factor=0.3, random_state=1, n_samples=4000, shuffle=True)\n",
    "#X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
    "X = scale(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\n",
    "sns.despine(); ax.legend()\n",
    "ax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_num_dense_nodes = Integer(low=2, high=40, name='num_dense_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [dim_num_dense_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_dense_nodes):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(num_dense_nodes, activation=tf.nn.sigmoid, input_shape=(2,)),\n",
    "    keras.layers.Dense(num_dense_nodes, activation=tf.nn.sigmoid),\n",
    "#    keras.layers.Dropout(rate=0.3),\n",
    "#    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "#    keras.layers.Dropout(rate=0.3),\n",
    "#    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "#    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(num_dense_nodes, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "# keep a list of tried parameter\n",
    "parameter_hist = []\n",
    "\n",
    "# keep a list of accuracy hist\n",
    "acc_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(num_dense_nodes):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "   \n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(num_dense_nodes=num_dense_nodes)\n",
    "\n",
    " \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=Y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=(X_test, Y_test)\n",
    "                       )\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # save parameter and accuracy history\n",
    "    parameter_hist.append(num_dense_nodes)\n",
    "    acc_hist.append(accuracy)\n",
    "    \n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "       \n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameter is 2 nodes per layer\n",
    "default_parameters = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 163us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.4865"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 160us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 205us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 23\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 196us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 163us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 34\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 159us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 164us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 174us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 234us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 240us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 188us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 32\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 161us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 196us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 26\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 325us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 10\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 314us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 4\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 312us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 21\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 316us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 196us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 179us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 36\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 163us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 165us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 194us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 189us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 20\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 325us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 22\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 136us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 179us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 180us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 317us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 310us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 311us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 308us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 310us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 312us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 2\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 316us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n",
      "num_dense_nodes: 40\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 310us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 7.7560 - acc: 0.5135 - val_loss: 8.1864 - val_acc: 0.4865\n",
      "\n",
      "Accuracy: 48.65%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#start searching hyper parameter with gp\n",
    "# we will do 11 call of fitness function, start with default parameters\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=22,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Result')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(parameter_hist, acc_hist)\n",
    "plt.title('Result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
